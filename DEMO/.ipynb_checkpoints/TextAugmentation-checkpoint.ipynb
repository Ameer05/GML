{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://test.pypi.org/simple/\n",
      "Collecting GML==3.1.4\n",
      "  Downloading https://test-files.pythonhosted.org/packages/13/d0/e5cf98e9b6e69bb71405cec2e25052881154cf32fef8bbe4114a94d312d0/GML-3.1.4-py3-none-any.whl (15.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.4 MB 5.1 MB/s eta 0:00:01    |███████████████████             | 9.1 MB 4.0 MB/s eta 0:00:02\n",
      "\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from GML==3.1.4) (1.6.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.7/site-packages (from GML==3.1.4) (4.9.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from GML==3.1.4) (4.45.0)\n",
      "Requirement already satisfied: lightgbm in /opt/conda/lib/python3.7/site-packages (from GML==3.1.4) (2.3.1)\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.7/site-packages (from GML==3.1.4) (0.10.0)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from GML==3.1.4) (3.2.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from GML==3.1.4) (0.23.2)\n",
      "Requirement already satisfied: xgboost in /opt/conda/lib/python3.7/site-packages (from GML==3.1.4) (1.2.1)\n",
      "Collecting efficientnet-pytorch\n",
      "  Downloading efficientnet_pytorch-0.7.0.tar.gz (20 kB)\n",
      "Requirement already satisfied: tensorflow in /opt/conda/lib/python3.7/site-packages (from GML==3.1.4) (2.3.1)\n",
      "Requirement already satisfied: albumentations in /opt/conda/lib/python3.7/site-packages (from GML==3.1.4) (0.5.1)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.7/site-packages (from GML==3.1.4) (1.5.1)\n",
      "Requirement already satisfied: catboost in /opt/conda/lib/python3.7/site-packages (from GML==3.1.4) (0.24.2)\n",
      "Collecting ftfy\n",
      "  Downloading ftfy-5.8.tar.gz (64 kB)\n",
      "\u001b[K     |████████████████████████████████| 64 kB 423 kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: Keras in /opt/conda/lib/python3.7/site-packages (from GML==3.1.4) (2.4.3)\n",
      "Collecting fastai==1.0.61\n",
      "  Downloading fastai-1.0.61-py3-none-any.whl (239 kB)\n",
      "\u001b[K     |████████████████████████████████| 239 kB 579 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: category-encoders in /opt/conda/lib/python3.7/site-packages (from GML==3.1.4) (2.2.2)\n",
      "Collecting string\n",
      "  Downloading https://test-files.pythonhosted.org/packages/7a/63/c48edcd5ab679d7e4873ccd540ccbfd29a82a565b4db2a055f4005e090ab/string-1.0.tar.gz (518 bytes)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from GML==3.1.4) (1.1.4)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from GML==3.1.4) (0.7.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from GML==3.1.4) (2.23.0)\n",
      "Collecting Pint\n",
      "  Downloading Pint-0.16.1-py2.py3-none-any.whl (205 kB)\n",
      "\u001b[K     |████████████████████████████████| 205 kB 744 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from GML==3.1.4) (1.18.5)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (from GML==3.1.4) (3.4.0)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->GML==3.1.4) (0.18.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.7/site-packages (from beautifulsoup4->GML==3.1.4) (1.9.4)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from lightgbm->GML==3.1.4) (1.4.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->GML==3.1.4) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->GML==3.1.4) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->GML==3.1.4) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->GML==3.1.4) (2.8.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->GML==3.1.4) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->GML==3.1.4) (0.14.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->GML==3.1.4) (2.3.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow->GML==3.1.4) (1.33.2)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow->GML==3.1.4) (1.1.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow->GML==3.1.4) (3.3.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /opt/conda/lib/python3.7/site-packages (from tensorflow->GML==3.1.4) (0.2.0)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->GML==3.1.4) (2.4.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorflow->GML==3.1.4) (0.34.2)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->GML==3.1.4) (0.11.0)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->GML==3.1.4) (2.10.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow->GML==3.1.4) (3.14.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow->GML==3.1.4) (1.6.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->GML==3.1.4) (1.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->GML==3.1.4) (1.14.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow->GML==3.1.4) (0.3.3)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow->GML==3.1.4) (1.11.2)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from albumentations->GML==3.1.4) (5.3.1)\n",
      "Requirement already satisfied: opencv-python-headless>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from albumentations->GML==3.1.4) (4.4.0.46)\n",
      "Requirement already satisfied: scikit-image>=0.16.1 in /opt/conda/lib/python3.7/site-packages (from albumentations->GML==3.1.4) (0.16.2)\n",
      "Requirement already satisfied: imgaug>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from albumentations->GML==3.1.4) (0.4.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.7/site-packages (from sympy->GML==3.1.4) (1.1.0)\n",
      "Requirement already satisfied: plotly in /opt/conda/lib/python3.7/site-packages (from catboost->GML==3.1.4) (4.12.0)\n",
      "Requirement already satisfied: graphviz in /opt/conda/lib/python3.7/site-packages (from catboost->GML==3.1.4) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from ftfy->GML==3.1.4) (0.1.9)\n",
      "Requirement already satisfied: nvidia-ml-py3 in /opt/conda/lib/python3.7/site-packages (from fastai==1.0.61->GML==3.1.4) (7.352.0)\n",
      "Requirement already satisfied: bottleneck in /opt/conda/lib/python3.7/site-packages (from fastai==1.0.61->GML==3.1.4) (1.3.2)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.7/site-packages (from fastai==1.0.61->GML==3.1.4) (8.0.1)\n",
      "Requirement already satisfied: numexpr in /opt/conda/lib/python3.7/site-packages (from fastai==1.0.61->GML==3.1.4) (2.7.1)\n",
      "Requirement already satisfied: spacy>=2.0.18; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from fastai==1.0.61->GML==3.1.4) (2.3.2)\n",
      "Requirement already satisfied: fastprogress>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from fastai==1.0.61->GML==3.1.4) (1.0.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from fastai==1.0.61->GML==3.1.4) (20.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: patsy>=0.5.1 in /opt/conda/lib/python3.7/site-packages (from category-encoders->GML==3.1.4) (0.5.1)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in /opt/conda/lib/python3.7/site-packages (from category-encoders->GML==3.1.4) (0.11.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas->GML==3.1.4) (2019.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->GML==3.1.4) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->GML==3.1.4) (2020.11.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->GML==3.1.4) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->GML==3.1.4) (1.25.9)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from Pint->GML==3.1.4) (1.6.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers->GML==3.1.4) (3.0.10)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers->GML==3.1.4) (2020.4.4)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in /opt/conda/lib/python3.7/site-packages (from transformers->GML==3.1.4) (0.1.94)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers->GML==3.1.4) (0.0.43)\n",
      "Requirement already satisfied: tokenizers==0.9.2 in /opt/conda/lib/python3.7/site-packages (from transformers->GML==3.1.4) (0.9.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow->GML==3.1.4) (0.4.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow->GML==3.1.4) (3.2.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow->GML==3.1.4) (1.14.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow->GML==3.1.4) (1.0.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow->GML==3.1.4) (1.7.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow->GML==3.1.4) (46.1.3.post20200325)\n",
      "Requirement already satisfied: networkx>=2.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations->GML==3.1.4) (2.4)\n",
      "Requirement already satisfied: imageio>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations->GML==3.1.4) (2.8.0)\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations->GML==3.1.4) (1.1.1)\n",
      "Requirement already satisfied: Shapely in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations->GML==3.1.4) (1.7.1)\n",
      "Requirement already satisfied: opencv-python in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations->GML==3.1.4) (4.4.0.46)\n",
      "Requirement already satisfied: retrying>=1.3.3 in /opt/conda/lib/python3.7/site-packages (from plotly->catboost->GML==3.1.4) (1.3.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai==1.0.61->GML==3.1.4) (1.0.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai==1.0.61->GML==3.1.4) (2.0.4)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai==1.0.61->GML==3.1.4) (0.8.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai==1.0.61->GML==3.1.4) (0.4.1)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai==1.0.61->GML==3.1.4) (1.0.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai==1.0.61->GML==3.1.4) (3.0.4)\n",
      "Requirement already satisfied: thinc==7.4.1 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai==1.0.61->GML==3.1.4) (7.4.1)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai==1.0.61->GML==3.1.4) (1.0.4)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/conda/lib/python3.7/site-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai==1.0.61->GML==3.1.4) (1.1.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->Pint->GML==3.1.4) (3.1.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers->GML==3.1.4) (7.1.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow->GML==3.1.4) (1.2.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->GML==3.1.4) (3.1.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->GML==3.1.4) (0.2.7)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->GML==3.1.4) (4.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx>=2.0->scikit-image>=0.16.1->albumentations->GML==3.1.4) (4.4.2)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow->GML==3.1.4) (3.0.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->GML==3.1.4) (0.4.8)\n",
      "Building wheels for collected packages: efficientnet-pytorch, ftfy, string\n",
      "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.0-py3-none-any.whl size=16035 sha256=91f9c530af8b386b18e4bacfd62a7846203d286fc4b2708f871cc35a878f7364\n",
      "  Stored in directory: /root/.cache/pip/wheels/b7/cc/0d/41d384b0071c6f46e542aded5f8571700ace4f1eb3f1591c29\n",
      "  Building wheel for ftfy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ftfy: filename=ftfy-5.8-py3-none-any.whl size=45612 sha256=a078e47115c595c2490d421617f49a9247910be04835c0941f2a863b3a5985fa\n",
      "  Stored in directory: /root/.cache/pip/wheels/49/1c/fc/8b19700f939810cd8fd9495ae34934b246279791288eda1c31\n",
      "  Building wheel for string (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for string: filename=string-1.0-py3-none-any.whl size=1027 sha256=b5ecf182d3b42ab883f9c1e7d6046fcc4b1d405d752c6b6a82bea639fcea342b\n",
      "  Stored in directory: /root/.cache/pip/wheels/5f/79/c4/8e2dd3efc8a8aa61808be4595eca947cbf1628109e1a2ca15d\n",
      "Successfully built efficientnet-pytorch ftfy string\n",
      "Installing collected packages: efficientnet-pytorch, ftfy, fastai, string, Pint, GML\n",
      "  Attempting uninstall: fastai\n",
      "    Found existing installation: fastai 2.0.19\n",
      "    Uninstalling fastai-2.0.19:\n",
      "      Successfully uninstalled fastai-2.0.19\n",
      "Successfully installed GML-3.1.4 Pint-0.16.1 efficientnet-pytorch-0.7.0 fastai-1.0.61 ftfy-5.8 string-1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --extra-index-url https://test.pypi.org/simple/ GML==3.1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ku7F_TeSgzer",
    "outputId": "51b4657a-7776-4435-e5b9-02671285a76b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GML is up to date \n",
      " \n",
      "\r\n",
      "\r\n",
      "Feel free to contact us at: https://github.com/Muhammad4hmed/Ghalat-Machine-Learning\n",
      "Your GML is ready!\n"
     ]
    }
   ],
   "source": [
    "from GML import AutoNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "huRXlK4abIRi"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../input/nlp-dataset-for-gml/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['ABSTRACT'].copy()\n",
    "y = data['Computer Science'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Yu_AO1KRbOYt"
   },
   "outputs": [],
   "source": [
    "nlp = AutoNLP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass each text individually to the clean function. but thats easy with pandas' apply function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arguments of clean function are as below:\n",
    "\n",
    "text, <br>\n",
    "fix_unicode=True, <br>\n",
    "to_ascii=True, <br>\n",
    "lower=True, <br>\n",
    "no_line_breaks=True, <br>\n",
    "no_urls=True, <br>\n",
    "no_emails=True, <br>\n",
    "no_phone_numbers=True, <br>\n",
    "no_numbers=True, <br>\n",
    "no_digits=True, <br>\n",
    "no_currency_symbols=True, <br>\n",
    "no_punct=True, <br>\n",
    "replace_with_url='<URL>', <br>\n",
    "replace_with_email='<EMAIL>', <br>\n",
    "replace_with_phone_number='<PHONE>', <br>\n",
    "replace_with_number='<NUMBER>', <br>\n",
    "replace_with_digit='0', <br>\n",
    "replace_with_currency_symbol='<CUR>', <br>\n",
    "lang='en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "XGl-IoyIbcW8"
   },
   "outputs": [],
   "source": [
    "cleanX = X.apply(lambda x: nlp.clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "dnoxZjmTbhGm",
    "outputId": "d8af6e14-8878-41a9-8af7-adf5d8763009"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ABSTRACT</th>\n",
       "      <th>Computer Science</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>predictive models allow subjectspecific infere...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rotation invariance and translation invariance...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>we introduce and develop the notion of spheric...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the stochastic landaulifshitzgilbert llg equat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fouriertransform infrared ftir spectra of samp...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20967</th>\n",
       "      <td>machine learning is finding increasingly broad...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20968</th>\n",
       "      <td>polycrystalline diamond coatings have been gro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20969</th>\n",
       "      <td>we present a new approach for identifying situ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20970</th>\n",
       "      <td>the sum of lognormal variates is encountered i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20971</th>\n",
       "      <td>recently optional stopping has been a subject ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20972 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                ABSTRACT  Computer Science\n",
       "0      predictive models allow subjectspecific infere...                 1\n",
       "1      rotation invariance and translation invariance...                 1\n",
       "2      we introduce and develop the notion of spheric...                 0\n",
       "3      the stochastic landaulifshitzgilbert llg equat...                 0\n",
       "4      fouriertransform infrared ftir spectra of samp...                 1\n",
       "...                                                  ...               ...\n",
       "20967  machine learning is finding increasingly broad...                 1\n",
       "20968  polycrystalline diamond coatings have been gro...                 0\n",
       "20969  we present a new approach for identifying situ...                 1\n",
       "20970  the sum of lognormal variates is encountered i...                 0\n",
       "20971  recently optional stopping has been a subject ...                 0\n",
       "\n",
       "[20972 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanX = pd.DataFrame(cleanX, columns = ['ABSTRACT'])\n",
    "cleanX = pd.concat([cleanX,data['Computer Science']], axis=1)\n",
    "cleanX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "DB7uve4zeroP"
   },
   "outputs": [],
   "source": [
    "def trunc_text(x):\n",
    "  if len(x) > 200:\n",
    "    return x[:200]\n",
    "  return x\n",
    "\n",
    "cleanX['ABSTRACT'] = cleanX['ABSTRACT'].apply(lambda x: trunc_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanX.to_csv('data.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arguments for augmentation_train are as follows:\n",
    "\n",
    "DATA_FILE: Path of the data file <br>\n",
    "BATCH_SIZE:  Batch size <br>\n",
    "EPOCHS: Number of augmentation epochs <br>\n",
    "LEARNING_RATE: Learning Rate <br>\n",
    "WARMUP_STEPS: Warm up steps <br>\n",
    "MAX_SEQ_LEN: Maximum sequence length in each of text <br>\n",
    "MODEL_NAME: For now, only gpt2 is supported. more will be added in future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading/Downloading GPT-2 Model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "344e9bb91d724cdf82788c574df469e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1042301.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "011e40f298384cd584799e203e82c07a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5537cad6ede14708899e5d07e486812e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=718.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b94b640f328b44199264cea42779dcdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1520013706.0, style=ProgressStyle(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running 1 epoch\n",
      "Total Loss is 104.73603820800781\n",
      "Total Loss is 112.86139678955078\n",
      "Total Loss is 118.34065246582031\n",
      "Total Loss is 145.15234375\n",
      "Total Loss is 156.99583435058594\n",
      "Total Loss is 152.2235870361328\n",
      "Total Loss is 151.24612426757812\n",
      "Total Loss is 153.5050506591797\n",
      "Total Loss is 152.9113311767578\n",
      "Total Loss is 151.2832794189453\n",
      "Total Loss is 146.8540802001953\n",
      "Total Loss is 144.33045959472656\n",
      "Total Loss is 142.19061279296875\n",
      "Total Loss is 145.0963134765625\n",
      "Total Loss is 147.210205078125\n",
      "Total Loss is 146.30068969726562\n",
      "Total Loss is 144.9857177734375\n",
      "Total Loss is 147.03077697753906\n",
      "Total Loss is 147.45782470703125\n",
      "Total Loss is 143.7101593017578\n",
      "Total Loss is 144.5182342529297\n",
      "Total Loss is 145.22311401367188\n",
      "Total Loss is 145.1485137939453\n",
      "Total Loss is 145.18280029296875\n",
      "Total Loss is 148.14932250976562\n",
      "Total Loss is 141.2455596923828\n",
      "Total Loss is 145.4959259033203\n",
      "Total Loss is 146.64236450195312\n",
      "Total Loss is 146.3128204345703\n",
      "Total Loss is 146.4281463623047\n",
      "Total Loss is 142.1925048828125\n",
      "Total Loss is 143.88687133789062\n",
      "Total Loss is 148.46185302734375\n",
      "Total Loss is 146.94544982910156\n",
      "Total Loss is 145.03781127929688\n",
      "Total Loss is 147.3777313232422\n",
      "Total Loss is 145.6553192138672\n",
      "Total Loss is 147.2757568359375\n",
      "Total Loss is 145.6207733154297\n",
      "Total Loss is 145.9559783935547\n",
      "Total Loss is 148.59255981445312\n",
      "Total Loss is 146.9285888671875\n",
      "Total Loss is 146.8224639892578\n",
      "Total Loss is 146.46734619140625\n",
      "Total Loss is 144.93560791015625\n",
      "Total Loss is 144.062744140625\n",
      "Total Loss is 145.44497680664062\n",
      "Total Loss is 145.25840759277344\n",
      "Total Loss is 147.41123962402344\n",
      "Total Loss is 142.80441284179688\n",
      "Total Loss is 146.57559204101562\n",
      "Total Loss is 145.5012969970703\n",
      "Total Loss is 145.37965393066406\n",
      "Total Loss is 145.70458984375\n",
      "Total Loss is 143.3974151611328\n",
      "Total Loss is 146.50526428222656\n",
      "Total Loss is 143.35911560058594\n",
      "Total Loss is 142.92315673828125\n",
      "Total Loss is 144.06881713867188\n",
      "Total Loss is 143.89895629882812\n",
      "Total Loss is 145.4073486328125\n",
      "Total Loss is 142.12820434570312\n",
      "Total Loss is 144.14974975585938\n",
      "Total Loss is 145.36241149902344\n",
      "Total Loss is 146.62728881835938\n",
      "Total Loss is 145.29783630371094\n",
      "Total Loss is 146.01344299316406\n",
      "Total Loss is 142.614501953125\n",
      "Total Loss is 148.58058166503906\n",
      "Total Loss is 142.3350372314453\n",
      "Total Loss is 147.1200408935547\n",
      "Total Loss is 144.70223999023438\n",
      "Total Loss is 143.85475158691406\n",
      "Total Loss is 142.39291381835938\n",
      "Total Loss is 147.4104461669922\n",
      "Total Loss is 142.92588806152344\n",
      "Total Loss is 144.19740295410156\n",
      "Total Loss is 143.62811279296875\n",
      "Total Loss is 149.4729461669922\n",
      "Total Loss is 145.65374755859375\n",
      "Total Loss is 144.41273498535156\n",
      "Total Loss is 149.55567932128906\n",
      "Total Loss is 145.6923370361328\n",
      "Total Loss is 144.38510131835938\n",
      "Total Loss is 146.47760009765625\n",
      "Total Loss is 146.47732543945312\n",
      "Total Loss is 144.14427185058594\n",
      "Total Loss is 144.9529571533203\n",
      "Total Loss is 146.76974487304688\n",
      "Total Loss is 145.4130401611328\n",
      "Total Loss is 147.14588928222656\n",
      "Total Loss is 147.9783935546875\n",
      "Total Loss is 145.3712158203125\n",
      "Total Loss is 148.6101531982422\n",
      "Total Loss is 143.77610778808594\n",
      "Total Loss is 145.48680114746094\n",
      "Total Loss is 146.92001342773438\n",
      "Total Loss is 147.2069091796875\n",
      "Total Loss is 144.17257690429688\n",
      "Total Loss is 146.35910034179688\n",
      "Total Loss is 140.34518432617188\n",
      "Total Loss is 143.37332153320312\n",
      "Total Loss is 144.52369689941406\n",
      "Total Loss is 144.00759887695312\n",
      "Total Loss is 145.5410919189453\n",
      "Total Loss is 145.1738739013672\n",
      "Total Loss is 149.31680297851562\n",
      "Total Loss is 144.5254669189453\n",
      "Total Loss is 147.8009490966797\n",
      "Total Loss is 145.89474487304688\n",
      "Total Loss is 144.9759979248047\n",
      "Total Loss is 143.84835815429688\n",
      "Total Loss is 141.41941833496094\n",
      "Total Loss is 145.1390380859375\n",
      "Total Loss is 150.08511352539062\n",
      "Total Loss is 146.74180603027344\n",
      "Total Loss is 145.84945678710938\n",
      "Total Loss is 145.39666748046875\n",
      "Total Loss is 146.86293029785156\n",
      "Total Loss is 144.96102905273438\n",
      "Total Loss is 146.4586639404297\n",
      "Total Loss is 144.97267150878906\n",
      "Total Loss is 145.24757385253906\n",
      "Total Loss is 147.42190551757812\n",
      "Total Loss is 143.636962890625\n",
      "Total Loss is 146.3711395263672\n",
      "Total Loss is 147.3195343017578\n",
      "Total Loss is 145.0715789794922\n",
      "Total Loss is 144.00302124023438\n",
      "Total Loss is 147.78208923339844\n",
      "Total Loss is 148.31912231445312\n",
      "Total Loss is 144.9552764892578\n",
      "Total Loss is 144.90086364746094\n",
      "Total Loss is 146.5382843017578\n",
      "Total Loss is 147.52659606933594\n",
      "Total Loss is 146.3914337158203\n",
      "Total Loss is 147.3661651611328\n",
      "Total Loss is 145.7345428466797\n",
      "Total Loss is 145.57940673828125\n",
      "Total Loss is 142.66769409179688\n",
      "Total Loss is 145.22645568847656\n",
      "Total Loss is 144.10118103027344\n",
      "Total Loss is 143.72061157226562\n",
      "Total Loss is 145.6617889404297\n",
      "Total Loss is 147.88526916503906\n",
      "Total Loss is 141.77932739257812\n",
      "Total Loss is 145.32089233398438\n",
      "Total Loss is 145.07908630371094\n",
      "Total Loss is 146.28724670410156\n",
      "Total Loss is 142.12303161621094\n",
      "Total Loss is 145.1352996826172\n",
      "Total Loss is 145.875\n",
      "Total Loss is 146.08042907714844\n",
      "Total Loss is 145.9125213623047\n",
      "Total Loss is 146.52151489257812\n",
      "Total Loss is 144.5623779296875\n",
      "Total Loss is 143.438232421875\n",
      "Total Loss is 147.61058044433594\n",
      "Total Loss is 148.02703857421875\n",
      "Total Loss is 146.77737426757812\n",
      "Total Loss is 142.6342315673828\n",
      "Total Loss is 146.0636749267578\n",
      "Total Loss is 142.4952392578125\n",
      "Total Loss is 144.10006713867188\n",
      "Total Loss is 141.1858367919922\n",
      "Total Loss is 148.75054931640625\n",
      "Total Loss is 144.02552795410156\n",
      "Total Loss is 148.46810913085938\n",
      "Total Loss is 146.98806762695312\n",
      "Total Loss is 143.71530151367188\n",
      "Total Loss is 144.12464904785156\n",
      "Total Loss is 144.71231079101562\n",
      "Total Loss is 146.79739379882812\n",
      "Total Loss is 146.8440399169922\n",
      "Total Loss is 142.47698974609375\n",
      "Total Loss is 144.88540649414062\n",
      "Total Loss is 142.9034423828125\n",
      "Total Loss is 145.6194610595703\n",
      "Total Loss is 144.37762451171875\n",
      "Total Loss is 150.27432250976562\n",
      "Total Loss is 145.19288635253906\n",
      "Total Loss is 142.77468872070312\n",
      "Total Loss is 143.9071502685547\n",
      "Total Loss is 145.6529998779297\n",
      "Total Loss is 148.85400390625\n",
      "Total Loss is 146.0426788330078\n",
      "Total Loss is 144.77725219726562\n",
      "Total Loss is 145.00985717773438\n",
      "Total Loss is 146.2347412109375\n",
      "Total Loss is 145.7513427734375\n",
      "Total Loss is 145.2029266357422\n",
      "Total Loss is 145.03610229492188\n",
      "Total Loss is 147.06044006347656\n",
      "Total Loss is 147.19891357421875\n",
      "Total Loss is 144.69432067871094\n",
      "Total Loss is 143.88597106933594\n",
      "Total Loss is 147.8527069091797\n",
      "Total Loss is 144.94412231445312\n",
      "Total Loss is 148.16111755371094\n",
      "Total Loss is 148.71995544433594\n",
      "Total Loss is 146.05174255371094\n",
      "Total Loss is 147.7614288330078\n",
      "Total Loss is 147.2187957763672\n",
      "Total Loss is 145.9312286376953\n",
      "Total Loss is 144.3688201904297\n",
      "Total Loss is 146.0614471435547\n",
      "Total Loss is 145.4281463623047\n",
      "Total Loss is 145.55601501464844\n",
      "Total Loss is 147.16543579101562\n",
      "Total Loss is 150.485107421875\n",
      "Total Loss is 145.61251831054688\n",
      "Total Loss is 149.17025756835938\n",
      "Total Loss is 145.1627197265625\n",
      "Total Loss is 145.1183319091797\n",
      "Total Loss is 146.4827880859375\n",
      "Total Loss is 146.5847625732422\n",
      "Total Loss is 144.6040802001953\n",
      "Total Loss is 146.4486846923828\n",
      "Total Loss is 143.3975372314453\n",
      "Total Loss is 145.99696350097656\n",
      "Total Loss is 143.79000854492188\n",
      "Total Loss is 145.8503875732422\n",
      "Total Loss is 144.9890899658203\n",
      "Total Loss is 146.16006469726562\n",
      "Total Loss is 147.265380859375\n",
      "Total Loss is 148.8063201904297\n",
      "Total Loss is 145.93313598632812\n",
      "Total Loss is 144.882568359375\n",
      "Total Loss is 144.41209411621094\n",
      "Total Loss is 144.6779327392578\n",
      "Total Loss is 144.72950744628906\n",
      "Total Loss is 147.86778259277344\n",
      "Total Loss is 148.18170166015625\n",
      "Total Loss is 145.77664184570312\n",
      "Total Loss is 146.38946533203125\n",
      "Total Loss is 149.6170196533203\n",
      "Total Loss is 142.0398712158203\n",
      "Total Loss is 144.61460876464844\n",
      "Total Loss is 146.7652587890625\n",
      "Total Loss is 147.7237091064453\n",
      "Total Loss is 144.3113555908203\n",
      "Total Loss is 147.99639892578125\n",
      "Total Loss is 147.72512817382812\n",
      "Total Loss is 146.6964874267578\n",
      "Total Loss is 146.33775329589844\n",
      "Total Loss is 147.74143981933594\n",
      "Total Loss is 145.70263671875\n",
      "Total Loss is 147.30697631835938\n",
      "Total Loss is 144.62281799316406\n",
      "Total Loss is 148.34869384765625\n",
      "Total Loss is 143.2584686279297\n",
      "Total Loss is 147.7154998779297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss is 145.56533813476562\n",
      "Total Loss is 147.0728759765625\n",
      "Total Loss is 146.5228729248047\n",
      "Total Loss is 144.0269317626953\n",
      "Total Loss is 144.80072021484375\n",
      "Total Loss is 144.91807556152344\n",
      "Total Loss is 143.52049255371094\n",
      "Total Loss is 145.4845733642578\n",
      "Total Loss is 147.95742797851562\n",
      "Total Loss is 144.84237670898438\n",
      "Total Loss is 144.19927978515625\n",
      "Total Loss is 146.91302490234375\n",
      "Total Loss is 144.40704345703125\n",
      "Total Loss is 146.3751983642578\n",
      "Total Loss is 145.97975158691406\n",
      "Total Loss is 145.73077392578125\n",
      "Total Loss is 145.08602905273438\n",
      "Total Loss is 143.26205444335938\n",
      "Total Loss is 147.15353393554688\n",
      "Total Loss is 144.78477478027344\n",
      "Total Loss is 147.36814880371094\n",
      "Total Loss is 144.28536987304688\n",
      "Total Loss is 143.64471435546875\n",
      "Total Loss is 147.72076416015625\n",
      "Total Loss is 144.70635986328125\n",
      "Total Loss is 145.41201782226562\n",
      "Total Loss is 147.35508728027344\n",
      "Total Loss is 144.3067626953125\n",
      "Total Loss is 144.3653564453125\n",
      "Total Loss is 148.10191345214844\n",
      "Total Loss is 146.0252685546875\n",
      "Total Loss is 146.24978637695312\n",
      "Total Loss is 145.8207244873047\n",
      "Total Loss is 148.1641082763672\n",
      "Total Loss is 146.32684326171875\n",
      "Total Loss is 144.1737060546875\n",
      "Total Loss is 146.61965942382812\n",
      "Total Loss is 144.70361328125\n",
      "Total Loss is 146.8356170654297\n",
      "Total Loss is 147.06576538085938\n",
      "Total Loss is 144.73255920410156\n",
      "Total Loss is 145.91893005371094\n",
      "Total Loss is 144.79933166503906\n",
      "Total Loss is 144.65029907226562\n",
      "Total Loss is 147.23765563964844\n",
      "Total Loss is 145.7270965576172\n",
      "Total Loss is 146.07876586914062\n",
      "Total Loss is 145.9300994873047\n",
      "Total Loss is 143.89166259765625\n",
      "Total Loss is 143.18521118164062\n",
      "Total Loss is 143.15133666992188\n",
      "Total Loss is 144.82113647460938\n",
      "Total Loss is 147.57627868652344\n",
      "Total Loss is 142.58761596679688\n",
      "Total Loss is 147.75747680664062\n",
      "Total Loss is 142.60809326171875\n",
      "Total Loss is 143.61795043945312\n",
      "Total Loss is 146.5682830810547\n",
      "Total Loss is 144.5252227783203\n",
      "Total Loss is 146.23097229003906\n",
      "Total Loss is 147.98712158203125\n",
      "Total Loss is 146.87332153320312\n",
      "Total Loss is 145.832275390625\n",
      "Total Loss is 144.19093322753906\n",
      "Total Loss is 145.681396484375\n",
      "Total Loss is 146.9715576171875\n",
      "Total Loss is 148.88638305664062\n",
      "Total Loss is 141.61685180664062\n",
      "Total Loss is 146.2149200439453\n",
      "Total Loss is 144.09719848632812\n",
      "Total Loss is 148.51934814453125\n",
      "Total Loss is 147.58123779296875\n",
      "Total Loss is 144.78948974609375\n",
      "Total Loss is 146.61639404296875\n",
      "Total Loss is 146.33856201171875\n",
      "Total Loss is 146.35931396484375\n",
      "Total Loss is 147.36566162109375\n",
      "Total Loss is 147.35324096679688\n",
      "Total Loss is 146.5191650390625\n",
      "Total Loss is 146.43246459960938\n",
      "Total Loss is 147.23348999023438\n",
      "Total Loss is 148.23951721191406\n",
      "Total Loss is 143.74798583984375\n",
      "Total Loss is 145.5048065185547\n",
      "Total Loss is 144.28781127929688\n",
      "Total Loss is 145.70413208007812\n",
      "Total Loss is 146.9565887451172\n",
      "Total Loss is 145.75811767578125\n",
      "Total Loss is 144.26393127441406\n",
      "Total Loss is 147.61817932128906\n",
      "Total Loss is 146.5675506591797\n",
      "Total Loss is 143.24252319335938\n",
      "Total Loss is 145.69125366210938\n",
      "Total Loss is 142.69131469726562\n",
      "Total Loss is 144.3011474609375\n",
      "Total Loss is 144.17759704589844\n",
      "Total Loss is 144.32676696777344\n",
      "Total Loss is 141.62222290039062\n",
      "Total Loss is 145.3770294189453\n",
      "Total Loss is 141.19647216796875\n",
      "Total Loss is 147.44764709472656\n",
      "Total Loss is 146.3075714111328\n",
      "Total Loss is 145.46444702148438\n",
      "Total Loss is 145.8205108642578\n",
      "Total Loss is 147.8713836669922\n",
      "Total Loss is 144.0043487548828\n",
      "Total Loss is 146.1716766357422\n",
      "Total Loss is 146.53884887695312\n",
      "Total Loss is 147.4720001220703\n",
      "Total Loss is 147.10211181640625\n",
      "Total Loss is 146.09957885742188\n",
      "Total Loss is 147.57730102539062\n",
      "Total Loss is 144.51564025878906\n",
      "Total Loss is 144.94422912597656\n",
      "Total Loss is 150.42703247070312\n",
      "Total Loss is 142.82139587402344\n",
      "Total Loss is 144.930419921875\n",
      "Total Loss is 146.1984405517578\n",
      "Total Loss is 146.50762939453125\n",
      "Total Loss is 147.8882293701172\n",
      "Total Loss is 146.2440185546875\n",
      "Total Loss is 147.25930786132812\n",
      "Total Loss is 141.72076416015625\n",
      "Total Loss is 142.61856079101562\n",
      "Total Loss is 143.25152587890625\n",
      "Total Loss is 145.66461181640625\n",
      "Total Loss is 150.30055236816406\n",
      "Total Loss is 144.8683624267578\n",
      "Total Loss is 145.8905792236328\n",
      "Total Loss is 143.28863525390625\n",
      "Total Loss is 144.9166259765625\n",
      "Total Loss is 141.11790466308594\n",
      "Total Loss is 144.84861755371094\n",
      "Total Loss is 148.7324981689453\n",
      "Total Loss is 146.7021484375\n",
      "Total Loss is 149.32131958007812\n",
      "Total Loss is 144.29971313476562\n",
      "Total Loss is 147.7177276611328\n",
      "Total Loss is 144.58058166503906\n",
      "Total Loss is 146.16363525390625\n",
      "Total Loss is 143.46646118164062\n",
      "Total Loss is 147.30262756347656\n",
      "Total Loss is 145.2611541748047\n",
      "Total Loss is 143.71014404296875\n",
      "Total Loss is 144.9232940673828\n",
      "Total Loss is 144.84132385253906\n",
      "Total Loss is 146.62158203125\n",
      "Total Loss is 145.03492736816406\n",
      "Total Loss is 148.43165588378906\n",
      "Total Loss is 145.78668212890625\n",
      "Total Loss is 146.09535217285156\n",
      "Total Loss is 148.94935607910156\n",
      "Total Loss is 144.4566650390625\n",
      "Total Loss is 145.39292907714844\n",
      "Total Loss is 147.8389892578125\n",
      "Total Loss is 144.67123413085938\n",
      "Total Loss is 144.87123107910156\n",
      "Total Loss is 144.6396942138672\n",
      "Total Loss is 147.29725646972656\n",
      "Total Loss is 146.351806640625\n",
      "Total Loss is 145.49435424804688\n",
      "Total Loss is 145.94281005859375\n",
      "Total Loss is 147.52227783203125\n",
      "Total Loss is 147.10340881347656\n",
      "Total Loss is 146.3921661376953\n",
      "Total Loss is 146.3505401611328\n",
      "Total Loss is 144.12667846679688\n",
      "Total Loss is 147.70668029785156\n",
      "Total Loss is 147.25381469726562\n",
      "Total Loss is 148.63238525390625\n",
      "Total Loss is 146.3390350341797\n",
      "Total Loss is 147.26878356933594\n",
      "Total Loss is 144.11907958984375\n",
      "Total Loss is 146.13340759277344\n",
      "Total Loss is 146.6284637451172\n",
      "Total Loss is 145.15032958984375\n",
      "Total Loss is 146.4105682373047\n",
      "Total Loss is 149.2736053466797\n",
      "Total Loss is 146.86607360839844\n",
      "Total Loss is 145.69944763183594\n",
      "Total Loss is 146.2353057861328\n",
      "Total Loss is 143.94712829589844\n",
      "Total Loss is 145.54298400878906\n",
      "Total Loss is 145.27610778808594\n",
      "Total Loss is 145.01422119140625\n",
      "Total Loss is 146.90606689453125\n",
      "Total Loss is 144.11021423339844\n",
      "Total Loss is 144.03099060058594\n",
      "Total Loss is 142.55523681640625\n",
      "Total Loss is 148.2818603515625\n",
      "Total Loss is 143.4046173095703\n",
      "Total Loss is 146.35951232910156\n",
      "Total Loss is 148.0059356689453\n",
      "Total Loss is 142.71046447753906\n",
      "Total Loss is 148.4733123779297\n",
      "Total Loss is 149.3545684814453\n",
      "Total Loss is 146.62985229492188\n",
      "Total Loss is 147.3924102783203\n",
      "Total Loss is 144.68765258789062\n",
      "Total Loss is 143.91127014160156\n",
      "Total Loss is 146.1730194091797\n",
      "Total Loss is 148.74249267578125\n",
      "Total Loss is 149.9208526611328\n",
      "Total Loss is 142.93182373046875\n",
      "Total Loss is 146.1355743408203\n",
      "Total Loss is 146.45773315429688\n",
      "Total Loss is 144.9443817138672\n",
      "Total Loss is 147.0950927734375\n",
      "Total Loss is 150.27110290527344\n",
      "Total Loss is 147.08505249023438\n",
      "Total Loss is 143.36294555664062\n",
      "Total Loss is 145.4528045654297\n",
      "Total Loss is 145.8441162109375\n",
      "Total Loss is 144.04734802246094\n",
      "Total Loss is 143.66305541992188\n",
      "Total Loss is 147.72573852539062\n",
      "Total Loss is 145.07577514648438\n",
      "Total Loss is 145.7855224609375\n",
      "Total Loss is 147.8115692138672\n",
      "Total Loss is 143.7727813720703\n",
      "Total Loss is 148.81283569335938\n",
      "Total Loss is 147.3169708251953\n",
      "Total Loss is 146.17311096191406\n",
      "Total Loss is 147.4154815673828\n",
      "Total Loss is 144.04371643066406\n",
      "Total Loss is 142.7718505859375\n",
      "Total Loss is 142.833740234375\n",
      "Total Loss is 146.20770263671875\n",
      "Total Loss is 150.0745391845703\n",
      "Total Loss is 148.56695556640625\n",
      "Total Loss is 144.3972930908203\n",
      "Total Loss is 145.74432373046875\n",
      "Total Loss is 147.19801330566406\n",
      "Total Loss is 146.40650939941406\n",
      "Total Loss is 144.22923278808594\n",
      "Total Loss is 147.44151306152344\n",
      "Total Loss is 145.0463409423828\n",
      "Total Loss is 145.2542724609375\n",
      "Total Loss is 147.12928771972656\n",
      "Total Loss is 151.14991760253906\n",
      "Total Loss is 146.32736206054688\n",
      "Total Loss is 148.55380249023438\n",
      "Total Loss is 145.97352600097656\n",
      "Total Loss is 148.52059936523438\n",
      "Total Loss is 148.20962524414062\n",
      "Total Loss is 142.71714782714844\n",
      "Total Loss is 143.30865478515625\n",
      "Total Loss is 144.18788146972656\n",
      "Total Loss is 146.3125\n",
      "Total Loss is 143.1852264404297\n",
      "Total Loss is 147.713623046875\n",
      "Total Loss is 145.0015869140625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss is 147.22215270996094\n",
      "Total Loss is 148.843994140625\n",
      "Total Loss is 143.0293731689453\n",
      "Total Loss is 145.16151428222656\n",
      "Total Loss is 144.7196044921875\n",
      "Total Loss is 143.7924041748047\n",
      "Total Loss is 144.43399047851562\n",
      "Total Loss is 146.88876342773438\n",
      "Total Loss is 148.25196838378906\n",
      "Total Loss is 145.48692321777344\n",
      "Total Loss is 143.2940673828125\n",
      "Total Loss is 145.84768676757812\n",
      "Total Loss is 143.39865112304688\n",
      "Total Loss is 145.1547393798828\n",
      "Total Loss is 147.31161499023438\n",
      "Total Loss is 147.78631591796875\n",
      "Total Loss is 149.0772705078125\n",
      "Total Loss is 141.44178771972656\n",
      "Total Loss is 146.46572875976562\n",
      "Total Loss is 148.19924926757812\n",
      "Total Loss is 142.69749450683594\n",
      "Total Loss is 147.31077575683594\n",
      "Total Loss is 145.34530639648438\n",
      "Total Loss is 146.34693908691406\n",
      "Total Loss is 146.961181640625\n",
      "Total Loss is 145.18360900878906\n",
      "Total Loss is 145.541748046875\n",
      "Total Loss is 151.1417236328125\n",
      "Total Loss is 146.98519897460938\n",
      "Total Loss is 146.0032501220703\n",
      "Total Loss is 144.64971923828125\n",
      "Total Loss is 145.5829620361328\n",
      "Total Loss is 149.8993377685547\n",
      "Total Loss is 148.32102966308594\n",
      "Total Loss is 146.67584228515625\n",
      "Total Loss is 145.70545959472656\n",
      "Total Loss is 145.37474060058594\n",
      "Total Loss is 144.9097137451172\n",
      "Total Loss is 147.19186401367188\n",
      "Total Loss is 143.56007385253906\n",
      "Total Loss is 145.24234008789062\n",
      "Total Loss is 147.32925415039062\n",
      "Total Loss is 144.23719787597656\n",
      "Total Loss is 146.7804718017578\n",
      "Total Loss is 144.68359375\n",
      "Total Loss is 145.84213256835938\n",
      "Total Loss is 146.0173797607422\n",
      "Total Loss is 142.1478729248047\n",
      "Total Loss is 146.48223876953125\n",
      "Total Loss is 146.0172576904297\n",
      "Total Loss is 147.9045867919922\n",
      "Total Loss is 145.9479217529297\n",
      "Total Loss is 146.93934631347656\n",
      "Total Loss is 144.1197052001953\n",
      "Total Loss is 146.85264587402344\n",
      "Total Loss is 147.99024963378906\n",
      "Total Loss is 141.02212524414062\n",
      "Total Loss is 145.7165069580078\n",
      "Total Loss is 145.43670654296875\n",
      "Total Loss is 146.6594696044922\n",
      "Total Loss is 144.4132843017578\n",
      "Total Loss is 145.6562042236328\n",
      "Total Loss is 144.98080444335938\n",
      "Total Loss is 147.9264678955078\n",
      "Total Loss is 148.77272033691406\n",
      "Total Loss is 144.163330078125\n",
      "Total Loss is 143.3246612548828\n",
      "Total Loss is 146.0085906982422\n",
      "Total Loss is 149.24081420898438\n",
      "Total Loss is 144.99017333984375\n",
      "Total Loss is 144.9736328125\n",
      "Total Loss is 146.10035705566406\n",
      "Total Loss is 145.21206665039062\n",
      "Total Loss is 141.63307189941406\n",
      "Total Loss is 145.24989318847656\n",
      "Total Loss is 143.45753479003906\n",
      "Total Loss is 147.754150390625\n",
      "Total Loss is 146.731689453125\n",
      "Total Loss is 144.40150451660156\n",
      "Total Loss is 143.89808654785156\n",
      "Total Loss is 146.86585998535156\n",
      "Total Loss is 146.24024963378906\n",
      "Total Loss is 145.75491333007812\n",
      "Total Loss is 144.05075073242188\n",
      "Total Loss is 149.4630889892578\n",
      "Total Loss is 146.55812072753906\n",
      "Total Loss is 144.5069580078125\n",
      "Total Loss is 145.18992614746094\n",
      "Total Loss is 147.1329345703125\n",
      "Total Loss is 147.07162475585938\n",
      "Total Loss is 142.48194885253906\n",
      "Total Loss is 145.57766723632812\n",
      "Total Loss is 142.23013305664062\n",
      "Total Loss is 147.1428680419922\n",
      "Total Loss is 145.0802001953125\n",
      "Total Loss is 143.58006286621094\n",
      "Total Loss is 146.24159240722656\n",
      "Total Loss is 147.00131225585938\n",
      "Total Loss is 147.85000610351562\n",
      "Total Loss is 146.82493591308594\n",
      "Total Loss is 143.63262939453125\n",
      "Total Loss is 147.9024200439453\n",
      "Total Loss is 143.16180419921875\n",
      "Total Loss is 146.82693481445312\n",
      "Total Loss is 143.6178741455078\n",
      "Total Loss is 146.89764404296875\n",
      "Total Loss is 144.32351684570312\n",
      "Total Loss is 143.5933380126953\n",
      "Total Loss is 144.11859130859375\n",
      "Total Loss is 146.2833251953125\n",
      "Total Loss is 147.33660888671875\n",
      "Total Loss is 149.55947875976562\n",
      "Total Loss is 142.332763671875\n",
      "Total Loss is 146.59072875976562\n",
      "Total Loss is 140.64739990234375\n",
      "Total Loss is 142.85101318359375\n",
      "Total Loss is 143.13157653808594\n",
      "Total Loss is 148.1952667236328\n",
      "Total Loss is 147.13821411132812\n",
      "Total Loss is 145.13015747070312\n",
      "Total Loss is 148.88172912597656\n",
      "Total Loss is 145.26858520507812\n",
      "Total Loss is 143.4298858642578\n",
      "Total Loss is 144.5579376220703\n",
      "Total Loss is 144.8186798095703\n",
      "Total Loss is 147.64979553222656\n",
      "Total Loss is 145.44989013671875\n",
      "Total Loss is 143.1476593017578\n",
      "Total Loss is 147.16204833984375\n",
      "Total Loss is 145.1624755859375\n",
      "Total Loss is 145.3336944580078\n",
      "Total Loss is 144.94833374023438\n",
      "Total Loss is 146.8472442626953\n",
      "Total Loss is 148.13088989257812\n",
      "Total Loss is 144.93569946289062\n",
      "Total Loss is 146.59043884277344\n",
      "Total Loss is 147.54000854492188\n",
      "Total Loss is 144.95201110839844\n",
      "Total Loss is 146.34156799316406\n",
      "Total Loss is 147.47361755371094\n",
      "Total Loss is 142.84141540527344\n",
      "Total Loss is 145.80638122558594\n",
      "Total Loss is 146.48724365234375\n",
      "Total Loss is 146.20159912109375\n",
      "Total Loss is 145.201904296875\n",
      "Total Loss is 146.20619201660156\n",
      "Total Loss is 143.78465270996094\n",
      "Total Loss is 148.15223693847656\n",
      "Total Loss is 142.07093811035156\n",
      "Total Loss is 146.09622192382812\n",
      "Total Loss is 146.03211975097656\n",
      "Total Loss is 146.90591430664062\n",
      "Total Loss is 149.04025268554688\n",
      "Total Loss is 144.44046020507812\n",
      "Total Loss is 145.72523498535156\n",
      "Total Loss is 144.52444458007812\n",
      "Total Loss is 145.2915802001953\n",
      "Total Loss is 145.5217742919922\n",
      "Total Loss is 145.33084106445312\n",
      "Total Loss is 144.14291381835938\n",
      "Total Loss is 150.48208618164062\n",
      "Total Loss is 149.11563110351562\n",
      "Total Loss is 147.21743774414062\n",
      "Total Loss is 146.7803955078125\n",
      "Total Loss is 145.02149963378906\n",
      "Total Loss is 146.1375732421875\n",
      "Total Loss is 146.4460906982422\n",
      "Total Loss is 145.00531005859375\n",
      "Total Loss is 147.70272827148438\n",
      "Total Loss is 145.6326446533203\n",
      "Total Loss is 145.1699676513672\n",
      "Total Loss is 143.03225708007812\n",
      "Total Loss is 147.51199340820312\n",
      "Total Loss is 141.0960693359375\n",
      "Total Loss is 143.8369903564453\n",
      "Total Loss is 146.1619415283203\n",
      "Total Loss is 145.7740936279297\n",
      "Total Loss is 148.5105438232422\n",
      "Total Loss is 146.35589599609375\n",
      "Total Loss is 146.49728393554688\n",
      "Total Loss is 146.2376708984375\n",
      "Total Loss is 147.02769470214844\n",
      "Total Loss is 145.83758544921875\n",
      "Total Loss is 142.3551788330078\n",
      "Total Loss is 145.5438690185547\n",
      "Total Loss is 143.11192321777344\n",
      "Total Loss is 147.6342010498047\n",
      "Total Loss is 147.12071228027344\n",
      "Total Loss is 142.5404052734375\n",
      "Total Loss is 145.60740661621094\n",
      "Total Loss is 144.72666931152344\n",
      "Total Loss is 146.1050567626953\n",
      "Total Loss is 143.92221069335938\n",
      "Total Loss is 141.69683837890625\n",
      "Total Loss is 146.8015594482422\n",
      "Total Loss is 147.29978942871094\n",
      "Total Loss is 146.87652587890625\n",
      "Total Loss is 146.56005859375\n",
      "Total Loss is 146.64529418945312\n",
      "Total Loss is 147.66612243652344\n",
      "Total Loss is 145.72476196289062\n",
      "Total Loss is 146.4497528076172\n",
      "Total Loss is 145.04176330566406\n",
      "Total Loss is 145.80284118652344\n",
      "Total Loss is 145.9733123779297\n",
      "Total Loss is 144.21112060546875\n",
      "Total Loss is 145.48367309570312\n",
      "Total Loss is 143.0012969970703\n",
      "Total Loss is 144.26824951171875\n",
      "Total Loss is 148.87318420410156\n",
      "Total Loss is 148.7440643310547\n",
      "Total Loss is 143.4563751220703\n",
      "Total Loss is 146.47442626953125\n",
      "Total Loss is 142.2161865234375\n",
      "Total Loss is 148.1685028076172\n",
      "Total Loss is 145.49725341796875\n",
      "Total Loss is 146.00611877441406\n",
      "Total Loss is 143.8220672607422\n",
      "Total Loss is 147.94430541992188\n",
      "Total Loss is 146.7123260498047\n",
      "Total Loss is 145.62826538085938\n",
      "Total Loss is 145.86007690429688\n",
      "Total Loss is 144.8331756591797\n",
      "Total Loss is 148.81207275390625\n",
      "Total Loss is 142.61248779296875\n",
      "Total Loss is 145.51199340820312\n",
      "Total Loss is 147.3713836669922\n",
      "Total Loss is 147.08877563476562\n",
      "Total Loss is 142.61447143554688\n",
      "Total Loss is 146.30523681640625\n",
      "Total Loss is 146.64210510253906\n",
      "Total Loss is 146.8464813232422\n",
      "Total Loss is 147.6572723388672\n",
      "Total Loss is 147.49220275878906\n",
      "Total Loss is 145.623291015625\n",
      "Total Loss is 143.1199951171875\n",
      "Total Loss is 148.332275390625\n",
      "Total Loss is 145.68772888183594\n",
      "Total Loss is 145.79722595214844\n",
      "Total Loss is 142.64466857910156\n",
      "Total Loss is 144.99798583984375\n",
      "Total Loss is 146.79217529296875\n",
      "Total Loss is 145.15945434570312\n",
      "Total Loss is 145.641845703125\n",
      "Total Loss is 149.93203735351562\n",
      "Total Loss is 147.51611328125\n",
      "Total Loss is 142.06800842285156\n",
      "Total Loss is 146.5966339111328\n",
      "Total Loss is 144.48873901367188\n",
      "Total Loss is 146.96548461914062\n",
      "Total Loss is 148.7209014892578\n",
      "Total Loss is 144.28829956054688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss is 147.8353271484375\n",
      "Total Loss is 144.48611450195312\n",
      "Total Loss is 149.8607177734375\n",
      "Total Loss is 144.8494415283203\n",
      "Total Loss is 146.64373779296875\n",
      "Total Loss is 147.32156372070312\n",
      "Total Loss is 145.68438720703125\n",
      "Total Loss is 147.64321899414062\n",
      "Total Loss is 146.42063903808594\n",
      "Total Loss is 148.8839111328125\n",
      "Total Loss is 151.4462890625\n",
      "Total Loss is 145.37550354003906\n",
      "Total Loss is 145.05722045898438\n",
      "Total Loss is 148.25234985351562\n",
      "Total Loss is 144.8623046875\n",
      "Total Loss is 143.7156524658203\n",
      "Total Loss is 147.4641876220703\n",
      "Total Loss is 145.24591064453125\n",
      "Total Loss is 146.6135711669922\n",
      "Total Loss is 145.6671905517578\n",
      "Total Loss is 143.7648162841797\n",
      "Total Loss is 144.48170471191406\n",
      "Total Loss is 141.7767791748047\n",
      "Total Loss is 147.59300231933594\n",
      "Total Loss is 144.73538208007812\n",
      "Total Loss is 146.84811401367188\n",
      "Total Loss is 146.92604064941406\n",
      "Total Loss is 146.73728942871094\n",
      "Total Loss is 145.59825134277344\n",
      "Total Loss is 145.1332244873047\n",
      "Total Loss is 145.2473907470703\n",
      "Total Loss is 144.66949462890625\n",
      "Total Loss is 145.3248291015625\n",
      "Total Loss is 147.0645751953125\n",
      "Total Loss is 142.26873779296875\n",
      "Total Loss is 143.3810577392578\n",
      "Total Loss is 144.88186645507812\n",
      "Total Loss is 146.4945526123047\n",
      "Total Loss is 143.04234313964844\n",
      "Total Loss is 146.31101989746094\n",
      "Total Loss is 145.06689453125\n",
      "Total Loss is 146.43785095214844\n",
      "Total Loss is 142.2384490966797\n",
      "Total Loss is 146.71168518066406\n",
      "Total Loss is 148.84507751464844\n",
      "Total Loss is 149.51051330566406\n",
      "Total Loss is 146.89715576171875\n",
      "Total Loss is 147.79481506347656\n",
      "Total Loss is 149.18222045898438\n",
      "Total Loss is 145.90638732910156\n",
      "Total Loss is 146.59341430664062\n",
      "Total Loss is 143.661376953125\n",
      "Total Loss is 147.97207641601562\n",
      "Total Loss is 146.89947509765625\n",
      "Total Loss is 144.45455932617188\n",
      "Total Loss is 143.6951446533203\n",
      "Total Loss is 146.7887725830078\n",
      "Total Loss is 143.50262451171875\n",
      "Total Loss is 143.59706115722656\n",
      "Total Loss is 145.09869384765625\n",
      "Total Loss is 147.67955017089844\n",
      "Total Loss is 145.0821533203125\n",
      "Total Loss is 144.7113494873047\n",
      "Total Loss is 145.30615234375\n",
      "Total Loss is 148.6268768310547\n",
      "Total Loss is 146.1901397705078\n",
      "Total Loss is 150.3830108642578\n",
      "Total Loss is 146.2295379638672\n",
      "Total Loss is 148.4149932861328\n",
      "Total Loss is 143.002685546875\n",
      "Total Loss is 149.22776794433594\n",
      "Total Loss is 146.3500518798828\n",
      "Total Loss is 147.39292907714844\n",
      "Total Loss is 146.42156982421875\n",
      "Total Loss is 147.16836547851562\n",
      "Total Loss is 147.08828735351562\n",
      "Total Loss is 146.05667114257812\n",
      "Total Loss is 148.15187072753906\n",
      "Total Loss is 145.64166259765625\n",
      "Total Loss is 143.1004180908203\n",
      "Total Loss is 146.66627502441406\n",
      "Total Loss is 144.43267822265625\n",
      "Total Loss is 145.91180419921875\n",
      "Total Loss is 148.5251922607422\n",
      "Total Loss is 148.30389404296875\n",
      "Total Loss is 144.64833068847656\n",
      "Total Loss is 144.9549102783203\n",
      "Total Loss is 144.56814575195312\n",
      "Total Loss is 144.04153442382812\n",
      "Total Loss is 144.99929809570312\n",
      "Total Loss is 146.8995819091797\n",
      "Total Loss is 145.22592163085938\n",
      "Total Loss is 147.0716552734375\n",
      "Total Loss is 142.4106903076172\n",
      "Total Loss is 145.4554443359375\n",
      "Total Loss is 147.61744689941406\n",
      "Total Loss is 146.30262756347656\n",
      "Total Loss is 144.5885772705078\n",
      "Total Loss is 144.86260986328125\n",
      "Total Loss is 147.7385711669922\n",
      "Total Loss is 144.7169952392578\n",
      "Total Loss is 146.7542266845703\n",
      "Total Loss is 148.28111267089844\n",
      "Total Loss is 146.15383911132812\n",
      "Total Loss is 145.33433532714844\n",
      "Total Loss is 144.53857421875\n",
      "Total Loss is 145.23573303222656\n",
      "Total Loss is 146.73834228515625\n",
      "Total Loss is 146.74864196777344\n",
      "Total Loss is 146.65585327148438\n",
      "Total Loss is 144.4017791748047\n",
      "Total Loss is 149.39048767089844\n",
      "Total Loss is 143.94924926757812\n",
      "Total Loss is 146.53244018554688\n",
      "Total Loss is 146.08255004882812\n",
      "Total Loss is 147.4193572998047\n",
      "Total Loss is 145.89927673339844\n",
      "Total Loss is 143.964599609375\n",
      "Total Loss is 147.0888671875\n",
      "Total Loss is 145.25071716308594\n",
      "Total Loss is 144.39016723632812\n",
      "Total Loss is 147.12144470214844\n",
      "Total Loss is 145.991455078125\n",
      "Total Loss is 148.9880828857422\n",
      "Total Loss is 145.4310760498047\n",
      "Total Loss is 142.22018432617188\n",
      "Total Loss is 143.20079040527344\n",
      "Total Loss is 148.65919494628906\n",
      "Total Loss is 148.64022827148438\n",
      "Total Loss is 143.7255859375\n",
      "Total Loss is 147.12689208984375\n",
      "Total Loss is 146.96493530273438\n",
      "Total Loss is 143.94732666015625\n",
      "Total Loss is 142.94773864746094\n",
      "Total Loss is 143.84634399414062\n",
      "Total Loss is 149.51242065429688\n",
      "Total Loss is 144.4381866455078\n",
      "Total Loss is 146.86209106445312\n",
      "Total Loss is 145.71240234375\n",
      "Total Loss is 143.14144897460938\n",
      "Total Loss is 144.8802032470703\n",
      "Total Loss is 144.31101989746094\n",
      "Total Loss is 140.9904022216797\n",
      "Total Loss is 146.59796142578125\n",
      "Total Loss is 145.67445373535156\n",
      "Total Loss is 146.52767944335938\n",
      "Total Loss is 147.9317626953125\n",
      "Total Loss is 147.95639038085938\n",
      "Total Loss is 146.33731079101562\n",
      "Total Loss is 144.94541931152344\n",
      "Total Loss is 143.7581787109375\n",
      "Total Loss is 148.01463317871094\n",
      "Total Loss is 143.33396911621094\n",
      "Total Loss is 148.18508911132812\n",
      "Total Loss is 142.84075927734375\n",
      "Total Loss is 146.4428253173828\n",
      "Total Loss is 147.96368408203125\n",
      "Total Loss is 146.64169311523438\n",
      "Total Loss is 145.22842407226562\n",
      "Total Loss is 144.84144592285156\n",
      "Total Loss is 144.21249389648438\n",
      "Total Loss is 144.09637451171875\n",
      "Total Loss is 146.1234588623047\n",
      "Total Loss is 146.30796813964844\n",
      "Total Loss is 147.27105712890625\n",
      "Total Loss is 146.26109313964844\n",
      "Total Loss is 144.6100616455078\n",
      "Total Loss is 145.32766723632812\n",
      "Total Loss is 146.95188903808594\n",
      "Total Loss is 148.50782775878906\n",
      "Total Loss is 146.6566619873047\n",
      "Total Loss is 145.0724334716797\n",
      "Total Loss is 145.0012664794922\n",
      "Total Loss is 145.5882568359375\n",
      "Total Loss is 145.94972229003906\n",
      "Total Loss is 147.84695434570312\n",
      "Total Loss is 147.11074829101562\n",
      "Total Loss is 148.70758056640625\n",
      "Total Loss is 145.08583068847656\n",
      "Total Loss is 147.116455078125\n",
      "Total Loss is 146.66712951660156\n",
      "Total Loss is 146.27252197265625\n",
      "Total Loss is 146.91200256347656\n",
      "Total Loss is 144.9391326904297\n",
      "Total Loss is 147.46083068847656\n",
      "Total Loss is 147.1591796875\n",
      "Total Loss is 145.15907287597656\n",
      "Total Loss is 147.75657653808594\n",
      "Total Loss is 145.48416137695312\n",
      "Total Loss is 148.92742919921875\n",
      "Total Loss is 144.69776916503906\n",
      "Total Loss is 146.65858459472656\n",
      "Total Loss is 145.99325561523438\n",
      "Total Loss is 142.69012451171875\n",
      "Total Loss is 146.3962860107422\n",
      "Total Loss is 148.86831665039062\n",
      "Total Loss is 146.75547790527344\n",
      "Total Loss is 143.5739288330078\n",
      "Total Loss is 146.69789123535156\n",
      "Total Loss is 145.75869750976562\n",
      "Total Loss is 146.62770080566406\n",
      "Total Loss is 146.3623809814453\n",
      "Total Loss is 149.06332397460938\n",
      "Total Loss is 143.80747985839844\n",
      "Total Loss is 144.66891479492188\n",
      "Total Loss is 146.7162322998047\n",
      "Total Loss is 147.0630340576172\n",
      "Total Loss is 148.573974609375\n",
      "Total Loss is 144.99874877929688\n",
      "Total Loss is 145.46871948242188\n",
      "Total Loss is 145.2816925048828\n",
      "Total Loss is 144.03802490234375\n",
      "Total Loss is 143.74566650390625\n",
      "Total Loss is 148.6555938720703\n",
      "Total Loss is 147.3741912841797\n",
      "Total Loss is 143.66661071777344\n",
      "Total Loss is 145.33924865722656\n",
      "Total Loss is 145.3518524169922\n",
      "Total Loss is 145.1527557373047\n",
      "Total Loss is 145.89297485351562\n",
      "Total Loss is 147.19142150878906\n",
      "Total Loss is 145.52980041503906\n",
      "Total Loss is 148.73233032226562\n",
      "Total Loss is 144.99583435058594\n",
      "Total Loss is 145.75784301757812\n",
      "Total Loss is 145.94436645507812\n",
      "Total Loss is 147.63262939453125\n",
      "Total Loss is 142.761962890625\n",
      "Total Loss is 146.7914276123047\n",
      "Total Loss is 145.83197021484375\n",
      "Total Loss is 143.60031127929688\n",
      "Total Loss is 143.9179229736328\n",
      "Total Loss is 146.59664916992188\n",
      "Total Loss is 146.10528564453125\n",
      "Total Loss is 143.2861785888672\n",
      "Total Loss is 146.3742218017578\n",
      "Total Loss is 146.29563903808594\n",
      "Total Loss is 145.480224609375\n",
      "Total Loss is 145.0482635498047\n",
      "Total Loss is 144.12783813476562\n",
      "Total Loss is 146.8246612548828\n",
      "Total Loss is 145.4727325439453\n",
      "Total Loss is 148.0026092529297\n",
      "Total Loss is 143.9413604736328\n",
      "Total Loss is 147.18325805664062\n",
      "Total Loss is 148.1897735595703\n",
      "Total Loss is 146.82623291015625\n",
      "Total Loss is 143.96072387695312\n",
      "Total Loss is 144.44552612304688\n",
      "Total Loss is 144.48641967773438\n",
      "Total Loss is 144.26329040527344\n",
      "Total Loss is 144.68321228027344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss is 144.53933715820312\n",
      "Total Loss is 146.39930725097656\n",
      "Total Loss is 149.6548309326172\n",
      "Total Loss is 144.54393005371094\n",
      "Total Loss is 144.87489318847656\n",
      "Total Loss is 147.4753875732422\n",
      "Total Loss is 145.5417022705078\n",
      "Total Loss is 150.4420166015625\n",
      "Total Loss is 145.3815460205078\n",
      "Total Loss is 146.30027770996094\n",
      "Total Loss is 143.62139892578125\n",
      "Total Loss is 146.18211364746094\n",
      "Total Loss is 145.48992919921875\n",
      "Total Loss is 145.67005920410156\n",
      "Total Loss is 146.4638214111328\n",
      "Total Loss is 144.7095489501953\n",
      "Total Loss is 150.85546875\n",
      "Total Loss is 146.89161682128906\n",
      "Total Loss is 143.7807159423828\n",
      "Total Loss is 144.1977081298828\n",
      "Total Loss is 144.78709411621094\n",
      "Total Loss is 144.08248901367188\n",
      "Total Loss is 144.79051208496094\n",
      "Total Loss is 149.5347442626953\n",
      "Total Loss is 142.75462341308594\n",
      "Total Loss is 144.0462188720703\n",
      "Total Loss is 152.28016662597656\n",
      "Total Loss is 146.12020874023438\n",
      "Total Loss is 146.62277221679688\n",
      "Total Loss is 143.45437622070312\n",
      "Total Loss is 143.90408325195312\n",
      "Total Loss is 147.60598754882812\n",
      "Total Loss is 145.99703979492188\n",
      "Total Loss is 143.0487823486328\n",
      "Total Loss is 141.7460479736328\n",
      "Total Loss is 143.28257751464844\n",
      "Total Loss is 145.64990234375\n",
      "Total Loss is 143.76824951171875\n",
      "Total Loss is 147.08409118652344\n",
      "Total Loss is 144.3843994140625\n"
     ]
    }
   ],
   "source": [
    "nlp.augmentation_train('./data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As augmentation model is trained, lets generate data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before generation, make sure to set params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.set_params(cleanX['ABSTRACT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arguments are:\n",
    "\n",
    "y: label column (numeric form) <br>\n",
    "SENTENCES: Sentences to generate for each of the label. eg, if there are 5 labels and SENTENCES = 10 then total 50 sentences will be generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [02:18<00:00, 69.29s/it]\n"
     ]
    }
   ],
   "source": [
    "new_Text = nlp.augmentation_generate(y = cleanX['Computer Science'], \n",
    "                                     SENTENCES = 100) # total 200 as we have 2 labels only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>! as ofistic or is and of problem for by more ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>! an data is an as we it as when and that as t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>! of play:  mostly  in mostly 0   0 are 0 0 we...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>! in an by is to various information on on ofn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>! or population an large with with and informa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>\" the the by via and and on as with d in and a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>\" sets this article more of long param a large...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>\" or the population with and density theistic ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>\" under mostly respect data information to a i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>\" is for work of for more in is in and as of o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text labels\n",
       "0    ! as ofistic or is and of problem for by more ...      0\n",
       "1    ! an data is an as we it as when and that as t...      0\n",
       "2    ! of play:  mostly  in mostly 0   0 are 0 0 we...      0\n",
       "3    ! in an by is to various information on on ofn...      0\n",
       "4    ! or population an large with with and informa...      0\n",
       "..                                                 ...    ...\n",
       "195  \" the the by via and and on as with d in and a...      1\n",
       "196  \" sets this article more of long param a large...      1\n",
       "197  \" or the population with and density theistic ...      1\n",
       "198  \" under mostly respect data information to a i...      1\n",
       "199  \" is for work of for more in is in and as of o...      1\n",
       "\n",
       "[200 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_Text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
